{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noway/VSCodeProjects/CodeTeX/codetex-backup/codetex/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 73 examples [00:00, 14051.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['code', 'equation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bleu = load(\"bleu\")\n",
    "\n",
    "# Load the dataset directly from a CSV file\n",
    "dataset = load_dataset('csv', data_files='data/dataset.csv')\n",
    "\n",
    "# Ensure the dataset has 'code' and 'equation' columns\n",
    "print(\"Dataset columns:\", dataset['train'].column_names)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "# First, split into train and test sets\n",
    "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Then, split the test set equally into validation and test sets\n",
    "test_valid = dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# Create a DatasetDict to hold the splits\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'validation': test_valid['train'],\n",
    "    'test': test_valid['test'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 5475.26 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 614.17 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 618.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the tokenizer and model\n",
    "model_name = \"t5-small\"  # You can replace this with another model like 't5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function using 'text_target' for target sequences\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['code'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False,  # Padding is handled by the data collator\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples['equation'],\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the tokenization function to the entire dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode the predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Post-process\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"bleu\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142903/3171033834.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      " 10%|█         | 9/90 [30:55<4:38:19, 206.17s/it]\n",
      " 10%|█         | 9/90 [00:09<01:07,  1.20it/s]/home/noway/VSCodeProjects/CodeTeX/codetex/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 10%|█         | 9/90 [00:10<01:07,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.665670394897461, 'eval_bleu': 0.3207024162225735, 'eval_runtime': 0.8512, 'eval_samples_per_second': 4.7, 'eval_steps_per_second': 1.175, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10/90 [00:11<01:27,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0574, 'grad_norm': 7.57383394241333, 'learning_rate': 1.7777777777777777e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 18/90 [00:18<00:55,  1.31it/s]\n",
      " 20%|██        | 18/90 [00:19<00:55,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.538629412651062, 'eval_bleu': 0.3207024162225735, 'eval_runtime': 0.8254, 'eval_samples_per_second': 4.846, 'eval_steps_per_second': 1.212, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/90 [00:20<01:08,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.046, 'grad_norm': 6.774435520172119, 'learning_rate': 1.555555555555556e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 27/90 [00:26<00:48,  1.30it/s]\n",
      " 30%|███       | 27/90 [00:27<00:48,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4278111457824707, 'eval_bleu': 0.3207024162225735, 'eval_runtime': 0.862, 'eval_samples_per_second': 4.64, 'eval_steps_per_second': 1.16, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 30/90 [00:30<00:56,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6193, 'grad_norm': 6.778203010559082, 'learning_rate': 1.3333333333333333e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 36/90 [00:35<00:43,  1.23it/s]\n",
      " 40%|████      | 36/90 [00:36<00:43,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.344699501991272, 'eval_bleu': 0.3207024162225735, 'eval_runtime': 0.8676, 'eval_samples_per_second': 4.61, 'eval_steps_per_second': 1.153, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 40/90 [00:39<00:45,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6512, 'grad_norm': 4.185177326202393, 'learning_rate': 1.1111111111111113e-05, 'epoch': 4.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 45/90 [00:43<00:38,  1.17it/s]\n",
      " 50%|█████     | 45/90 [00:44<00:38,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2810853719711304, 'eval_bleu': 0.2984942907959129, 'eval_runtime': 0.8326, 'eval_samples_per_second': 4.804, 'eval_steps_per_second': 1.201, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 50/90 [00:49<00:41,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7885, 'grad_norm': 5.181885242462158, 'learning_rate': 8.888888888888888e-06, 'epoch': 5.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 54/90 [00:53<00:30,  1.16it/s]\n",
      " 60%|██████    | 54/90 [00:54<00:30,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.239700198173523, 'eval_bleu': 0.2984942907959129, 'eval_runtime': 0.9128, 'eval_samples_per_second': 4.382, 'eval_steps_per_second': 1.096, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 60/90 [01:00<00:31,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5811, 'grad_norm': 5.7886128425598145, 'learning_rate': 6.666666666666667e-06, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 63/90 [01:02<00:22,  1.22it/s]\n",
      " 70%|███████   | 63/90 [01:03<00:22,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2039539813995361, 'eval_bleu': 0.25185193909177644, 'eval_runtime': 0.8521, 'eval_samples_per_second': 4.694, 'eval_steps_per_second': 1.174, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 70/90 [01:09<00:17,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4856, 'grad_norm': 4.890051364898682, 'learning_rate': 4.444444444444444e-06, 'epoch': 7.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 72/90 [01:10<00:13,  1.34it/s]\n",
      " 80%|████████  | 72/90 [01:11<00:13,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1835471391677856, 'eval_bleu': 0.2458423950688514, 'eval_runtime': 0.7949, 'eval_samples_per_second': 5.032, 'eval_steps_per_second': 1.258, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 80/90 [01:19<00:10,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6199, 'grad_norm': 12.634102821350098, 'learning_rate': 2.222222222222222e-06, 'epoch': 8.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 81/90 [01:19<00:07,  1.15it/s]\n",
      " 90%|█████████ | 81/90 [01:20<00:07,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1701003313064575, 'eval_bleu': 0.2458423950688514, 'eval_runtime': 0.9728, 'eval_samples_per_second': 4.112, 'eval_steps_per_second': 1.028, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [01:28<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.539, 'grad_norm': 7.847878932952881, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noway/VSCodeProjects/CodeTeX/codetex/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|██████████| 90/90 [01:31<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1647580862045288, 'eval_bleu': 0.2458423950688514, 'eval_runtime': 1.0262, 'eval_samples_per_second': 3.898, 'eval_steps_per_second': 0.975, 'epoch': 10.0}\n",
      "{'train_runtime': 91.4725, 'train_samples_per_second': 7.106, 'train_steps_per_second': 0.984, 'train_loss': 1.7097603585984973, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('t5-code-to-math/tokenizer_config.json',\n",
       " 't5-code-to-math/special_tokens_map.json',\n",
       " 't5-code-to-math/tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"t5-code-to-math\")\n",
    "tokenizer.save_pretrained(\"t5-code-to-math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noway/VSCodeProjects/CodeTeX/codetex/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00, 129.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.0083293914794922, 'eval_bleu': 0.38694621926276307, 'eval_runtime': 0.9791, 'eval_samples_per_second': 4.086, 'eval_steps_per_second': 1.021, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Evaluation results:\", results)\n",
    "\n",
    "# Prepare the model for inference\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "equation_generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code:\n",
      "def add(a, b): return a + b\n",
      "Generated Equation: def add(a, b)\n",
      "\n",
      "Code:\n",
      "def multiply(x, y):\n",
      "    return x * y\n",
      "Generated Equation: multiply(x, y)\n",
      "\n",
      "Code:\n",
      "def divide(numerator, denominator):\n",
      "    return numerator / denominator\n",
      "Generated Equation: def divide(numerator, denominator): return numerator / denominator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_equations(code_snippets, max_length=256, num_beams=4, early_stopping=True):\n",
    "    \"\"\"\n",
    "    Generates equations from a list of code snippets.\n",
    "\n",
    "    Args:\n",
    "        code_snippets (list of str): List of code snippets to convert.\n",
    "        max_length (int, optional): Maximum length of the generated equation. Defaults to 256.\n",
    "        num_beams (int, optional): Number of beams for beam search. Defaults to 4.\n",
    "        early_stopping (bool, optional): Whether to stop the beam search when at least `num_beams` sentences are finished. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Generated equations corresponding to the input code snippets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(code_snippets, list):\n",
    "            raise ValueError(\"Input should be a list of code snippets.\")\n",
    "\n",
    "        # Generate equations using the pipeline\n",
    "        generated = equation_generator(\n",
    "            code_snippets,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=early_stopping,\n",
    "            truncation=True  # Ensure inputs are truncated to the model's max input length\n",
    "        )\n",
    "\n",
    "        # Extract the generated text\n",
    "        equations = [item['generated_text'] for item in generated]\n",
    "        return equations\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during equation generation: {e}\")\n",
    "        return []\n",
    "    \n",
    "code_examples = [\n",
    "    \"def add(a, b): return a + b\",\n",
    "    \"def multiply(x, y):\\n    return x * y\",\n",
    "    \"def divide(numerator, denominator):\\n    return numerator / denominator\"\n",
    "]\n",
    "\n",
    "generated_equations = generate_equations(code_examples)\n",
    "for code, eq in zip(code_examples, generated_equations):\n",
    "    print(f\"Code:\\n{code}\\nGenerated Equation: {eq}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
